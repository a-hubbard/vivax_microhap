---
title: Properties of Pv4 Samples After Variant Filtering
author: Alfred Hubbard
output:
  html_notebook: default
  pdf_document: default
  github_document: default
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
# Load required libraries ----------------------------------------------
# These will be referenced without the `package::` construct, and thus 
# are loaded second to avoid masking
library(knitr)
library(magrittr)
library(sf)
library(tidyverse)

# Don't include code in PDF output
opts_chunk$set(echo = ! is_latex_output(), out.width = "100%")
# Set default ggplot2 theme
theme_set(theme_bw())

# Hardcode parameters. These are not passed as shell arguments because 
# hardcoded values facilitate development and this document is not 
# meant to be usable with other projects.
inp <- list(
  sample_metadata = "../../resources/MalariaGEN/orig/Pv4_samples.txt", 
  sample_coi = "../../resources/MalariaGEN/orig/Pv4_fws.txt", 
  sample_missingness = "../../results/MalariaGEN/preprocessed/missbysamp.tsv"
)
out <- list(
  filtered_samples = "../../results/filtered_samples.txt", 
  filtered_metadata = 
    "../../results/MalariaGEN/preprocessed/sample_metadata.csv", 
  study_sites = "../../results/unfiltered_sites.geojson"
)
```

# Proportion of Missingness by Sample

```{r}
# Read in and join sample information ----------------------------------
metadata <- read_tsv(
  inp$sample_metadata, 
  col_types = cols(
    .default = col_character(), 
    Lat = col_double(), 
    Long = col_double(), 
    Year = col_integer(), 
    `% callable` = col_double(), 
    `QC pass` = col_logical(), 
    `Is returning traveller` = col_logical()
  ), 
  progress = FALSE
)
coi <- read_tsv(
  inp$sample_coi, 
  col_types = cols(
    .default = col_character(), 
    Fws = col_double()
  ), 
  progress = FALSE
)
prop_miss <- read_tsv(
    inp$sample_missingness, 
    col_types = cols(
      .default = col_integer(), 
      INDV = col_character(), 
      F_MISS = col_double()
    ), 
    progress = FALSE
  ) %>%
  select(INDV, F_MISS)
sample_info <- left_join(metadata, coi, by = "Sample") %>%
  left_join(prop_miss, by = c("Sample" = "INDV")) %>%
  rename(sample_id = Sample)

# Filter samples based on QC, polyclonality, and longitudinal studies --
sample_info_initialfilter <- sample_info %>%
  filter(`QC pass`) %>%
  # Curiously, there are 41 samples without Fws values after the ones 
  # that failed QC are removed - the documentation says Fws was 
  # calculated for all QC pass samples
  filter(! is.na(Fws)) %>%
  # Remove polyclonal samples
  filter(Fws >= 0.95) %>%
  # Remove longitudinal samples
  filter(! str_detect(`All samples same individual`, ",")) %>%
  # Remove returning travelers
  filter(! `Is returning traveller`) %>%
  select(
    -`QC pass`, 
    -`All samples same individual`, 
    -`Exclusion reason`, 
    -`Is returning traveller`
  )

# Plot missingness -----------------------------------------------------
sample_info_initialfilter %>%
  ggplot(mapping = aes(x = F_MISS)) +
  geom_histogram(bins = 20) +
  labs(x = "Proportion of Missing Data", y = "No. of Samples")
```

This histogram shows the proportions of missing variants in each sample, once
samples that fail QC, polyclonal samples, and samples from longitudinal studies
are removed. The proportions of missing data in these remaining samples are
quite low, so no filter for missing data is necessary.

# Sample Distribution by Year

```{r}
# Plot sample size by year ---------------------------------------------
sample_info_initialfilter %>%
  ggplot(mapping = aes(x = Year)) +
  geom_bar() +
  labs(y = "No. of Samples")
```

This bar plot shows the sample count by year.

```{r}
# Zoom in on recent data -----------------------------------------------
sample_info_initialfilter %>%
  filter(Year >= 2010) %>%
  ggplot(mapping = aes(x = Year)) +
  geom_bar() +
  facet_wrap(vars(Population)) +
  labs(y = "No. of Samples")
```

This plot shows the same thing, with samples from before 2010 removed. In
addition, the plots have been facetted by region.

From this figure, it seems that Africa, Southeast Asia, and Latin America may
all have suitably dense sample sets. These are visualized in more detail below.

```{r}
# Zoom in on recent data -----------------------------------------------
sample_info_initialfilter %>%
  filter(Year >= 2010, Population %in% c("AF", "WSEA", "ESEA", "LAM")) %>%
  ggplot(mapping = aes(x = Year, fill = Country)) +
  geom_bar(position = "stack") +
  facet_wrap(vars(Population)) +
  labs(y = "No. of Samples")
```

As above, except limited to Africa, Southeast Asia, and Latin America. Bars are
now stacked, with colors scaled by country.

From this, three clusters in space and time present themselves as likely
populations for paneljudge analysis: Vietnam and Cambodia in 2015 and 2016;
Ethiopia in 2013; and Brazil, Colombia, and Peru in 2013 and 2014.

```{r}
sample_info_filterbyyear <- sample_info_initialfilter %>%
  filter(
    (Population == "ESEA" & Year %in% c(2015, 2016)) |
    (Population == "LAM" & Year %in% c(2013, 2014)) |
    (Country == "Ethiopia" & Year == 2013)
  )
```

# Sample Sizes

In the final analysis set, this leaves us with a sample size of `r 
nrow(sample_info_filterbyyear)`. The sample sizes for each site are given in the 
following table:

```{r}
# Compute sample sizes by country and site -----------------------------
sample_info_filterbyyear %>%
  group_by(Country, Site) %>%
  summarize(n_samp = n(), .groups = "drop")
```

# Study Information

The following table shows which studies generated the data in the final analysis
set.

```{r}
# Print studies present in filtered data -------------------------------
sample_info_filterbyyear %>%
  group_by(Study, Site, Country, Year) %>%
  summarize(n_samp = n(), .groups = "drop")

# Change spelling of Ho Chi Minh ---------------------------------------
sample_info_filterbyyear <- sample_info_filterbyyear %>%
  mutate(Site = str_replace(Site, "Ho Chi Min", "Ho Chi Minh"))

# Write sample information to disk -------------------------------------
sample_info_filterbyyear %>%
  select(sample_id) %>%
  write_csv(out$filtered_samples, col_names = FALSE)
write_csv(sample_info_filterbyyear, out$filtered_metadata)

# Write GeoJSON of study sites to disk ---------------------------------
sample_info_filterbyyear %>%
  group_by(Country, Site, Lat, Long) %>%
  summarize(n_samp = n(), .groups = "drop") %>%
  st_as_sf(coords = c("Long", "Lat")) %>%
  st_set_crs(4326) %>%
  st_write(out$study_sites, delete_dsn = TRUE)
```
