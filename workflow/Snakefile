import glob
import os


def expand_w_samp_list(wildcards, expression):
    """Read in list of samples and use to expand provided expression.

    This function reads the output of inspect_sample_properties to get 
    the list of samples selected for analysis. It then uses this list 
    to expand the provided wildcard expression.

    Arguments:
    wildcards -- Snakemake wildcards object
    expression -- Either a string or a list containing one or more 
        wildcard expressions containing the wildcard "samp"
    """
    samp_txt = checkpoints.inspect_sample_properties.get(**wildcards).output.\
        samp_list
    samp_l = []
    with open(samp_txt, 'r') as samp_f:
        for line in samp_f:
            samp_l.append(line.rstrip())
    return(expand(expression,samp=samp_l))


# Identify rules to be executed locally
localrules:
    refmt_primers, 
    dwnld_ref, 
    unzip_ref, 
    index_ref_bwa, 
    primers2fastq, 
    find_sa_coords, 
    map_primers, 
    sam2bam, 
    bam2bed, 
    captured_seq_bounds, 
    extract_captured_seq, 
    investigate_mapping, 
    collate_may2022_readcounts_metadata, 
    assess_marker_performance, 
    filter_bed, 
    dwnld_metadata, 
    dwnld_coi, 
    index_variant_file, 
    filter_bcf_by_marker, 
    concat_bcfs, 
    filter_vcf_by_snpqc, 
    compute_missbysamp, 
    inspect_sample_properties, 
    filter_samps, 
    apply_variants, 
    filter2hapseq, 
    alignment2hap, 
    csv2genind, 
    estimate_diversity, 
    assess_ld, 
    assess_selection, 
    filter4popgen, 
    analyze_relatedness, 
    plot_panel_marker_metrics, 
    plot_otreads_by_protocol, 
    plot_marker_reads_by_protocol, 
    plot_otreads_vs_density, 
    push_results_notebooks, 
    push_figs, 
    clean

# Notebooks
rendered_notebook_dir = os.path.join("results", "notebooks")
notebooks = {}
notebook_basenames = [
    "investigate_mapping", 
    "marker_performance", 
    "sample_properties", 
    "diversity", 
    "ld", 
    "selection", 
    "relatedness"]
for nb in notebook_basenames:
    notebooks[nb] = os.path.join(rendered_notebook_dir, nb)
nb_flag_template = os.path.join(".flags", "push_nb_{nb}")
# Figures
# All figures, even supplemental ones, are defined here, in the same 
# fashion, to facilitate easy swapping of inserts between supplemental 
# material and the main document
figdir = os.path.join("results", "figs")
figs = {
    "panel_marker_metrics": "fig1", 
    "otreads_by_protocol": "fig2", 
    "marker_reads_by_protocol": "fig3", 
    "otreads_vs_density": "fig4"}
fig_flag_template = os.path.join(".flags", "push_fig_{fig}")
microhap_dir = os.path.join("results", "microhap")
mg_microhap_dir = os.path.join(microhap_dir, "MalariaGEN")
mg_microhap_aligned_fasta_dir = os.path.join(mg_microhap_dir, "aligned_fastas")
rule all:
    input:
        expand("{nb}.{ext}", nb=list(notebooks.values()), ext=["pdf","md"]), 
        expand(nb_flag_template, nb=notebook_basenames), 
        expand(os.path.join(figdir,"{fig}.png"), fig=list(figs.values())), 
        expand(fig_flag_template, fig=list(figs.values())), 
        mg_microhap_aligned_fasta_dir
# For others attempting to run the pipeline on their own machine
rule replicate:
    input:
        expand(
            "{nb}.{ext}", 
            nb=list(notebooks.values()), 
            ext=["nb.html","pdf","md"]), 
        expand(os.path.join(figdir,"{fig}.{ext}"), fig=figs, ext=["png","pdf"])

# Reformat and filter primers
sd_id_file = os.path.join("resources", "id_file.txt")
primer_info = os.path.join(
    "resources", 
    "Microhaplotype primers_fixedRCpf_2023-03-02.xlsx")
primers_tsv = os.path.join("results", "primers.tsv")
fwd_primers = os.path.join("results", "fwd_primers.fasta")
rev_primers = os.path.join("results", "rev_primers.fasta")
rule refmt_primers:
    input:
        sd_id_file = sd_id_file, 
        primer_info = primer_info, 
        code = os.path.join("workflow", "scripts", "refmt_primers.R")
    output:
        primers_tsv = primers_tsv, 
        fwd_primers = fwd_primers, 
        rev_primers = rev_primers
    shell:
        "Rscript {input.code} --sd_id_file {input.sd_id_file} --primer_info "
        "'{input.primer_info}' --primers_tsv {output.primers_tsv} "
        "--fwd_primers {output.fwd_primers} --rev_primers {output.rev_primers}"

# Get primer locations and target reference sequences
# Download reference genome
ref_genome_zip = os.path.join("results", "GCA_900093555.2.zip")
rule dwnld_ref:
    output: temp(ref_genome_zip)
    shell: 
        "datasets download genome accession GCA_900093555.2 --filename {output}"
ref_genome_dir = os.path.join("results", "ref_genome")
ref_genome = os.path.join(
    ref_genome_dir, 
    "GCA_900093555.2", 
    "GCA_900093555.2_GCA_900093555_genomic.fna")
rule unzip_ref:
    input: ref_genome_zip
    output: 
        ref_genome, 
        os.path.join(ref_genome_dir, "README.md"), 
        os.path.join(ref_genome_dir, "assembly_data_report.jsonl"), 
        os.path.join(ref_genome_dir, "dataset_catalog.json")
    shell:
        """
        unzip {input} -d {ref_genome_dir}
        mv {ref_genome_dir}/ncbi_dataset/data/* {ref_genome_dir}/
        rm -r {ref_genome_dir}/ncbi_dataset
        """
# Necessary for mapping
ref_genome_index = multiext(ref_genome, ".amb", ".ann", ".bwt", ".pac", ".sa")
rule index_ref_bwa:
    input: ref_genome
    output: ref_genome_index
    shell: "bwa index {input}"
# Create FASTQ with primer sequences
prim_map_dir = os.path.join("results", "primer_mapping")
primers_fq = os.path.join(prim_map_dir, "primers.fastq")
rule primers2fastq:
    input: primers_tsv
    output: primers_fq
    run:
        with open(input[0], 'r') as primer_list:
            with open(output[0], 'w') as out_fq:
                # Skip header
                primer_list.readline()
                for line in primer_list:
                    elements = line.rstrip().split()
                    target = elements[0]
                    fwd_primer = elements[1]
                    rev_primer = elements[2]
                    out_fq.write("@" + target + "_forward\n")
                    out_fq.write(fwd_primer + '\n')
                    out_fq.write("+\n")
                    out_fq.write("~" * len(fwd_primer) + '\n')
                    out_fq.write("@" + target + "_reverse\n")
                    out_fq.write(rev_primer + '\n')
                    out_fq.write("+\n")
                    out_fq.write("~" * len(rev_primer) + '\n')
# In the next two steps, aln and samse are used to employ the BWA-
# backtrack algorithm, which is meant for sequences less than 100 bp, 
# and especially less than 70 bp
sa_coords = os.path.join(prim_map_dir, "mapping.sai")
rule find_sa_coords:
    input: 
        ref_genome_index, 
        ref = ref_genome, 
        query = primers_fq
    output: sa_coords
    shell: "bwa aln {input.ref} {input.query} > {output}"
primer_mapping_sam = sa_coords[:-3] + "sam"
rule map_primers:
    input:
        ref_genome_index, 
        ref = ref_genome, 
        query = primers_fq, 
        sa_coords = sa_coords
    output: primer_mapping_sam
    shell: "bwa samse {input.ref} {input.sa_coords} {input.query} > {output}"
rule sam2bam:
    input: "{filename}.sam"
    output: temp("{filename}.bam")
    shell: "samtools view -h -b {input} > {output}"
rule bam2bed:
    input: "{filename}.bam"
    output: "{filename}.bed"
    shell: "bedtools bamtobed -i {input} > {output}"
# Convert primer coordinates to target coordinates, both with and 
# without primers
primer_mapping_bed = primer_mapping_sam[:-3] + "bed"
trg_coords_wprimers = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_wprimers.bed")
trg_coords_noprimers = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_noprimers.bed")
rule captured_seq_bounds:
    input: primer_mapping_bed
    output: 
        coords_wprimers = trg_coords_wprimers, 
        coords_noprimers = trg_coords_noprimers
    run:
        with open(input[0], 'r') as primer_coord_f:
            with open(output.coords_wprimers, 'w') as trg_coord_wprimers_f, \
                    open(output.coords_noprimers, 'w') as trg_coord_noprimers_f:
                # The following logic assumes the bed file is sorted 
                # such that forward primers are before reverse primers 
                # and all hits for a given primer pair are grouped 
                # together. Due to the small size of the bed file in 
                # question, this assumption was only verified by 
                # visual inspection.
                chrom = ""
                target = ""
                for line in primer_coord_f:
                    elements = line.rstrip().split()
                    if elements[3].split('_')[-1] == "forward":
                        if elements[3][:-8] == target:
                            raise ValueError(
                                "Forward primer for " + elements[3][:-8] +\
                                " is a duplicate")
                        chrom = elements[0]
                        target = elements[3][:-8]
                        fwd_start = int(elements[1])
                        fwd_end = int(elements[2])
                    if elements[3].split('_')[-1] == "reverse":
                        if elements[3][:-8] != target:
                            raise ValueError(
                                "Reverse primer for " + elements[3][:-8] +\
                                " does not match previous forward primer, " +\
                                "or is a duplicate")
                        elif elements[0] != chrom:
                            raise ValueError(
                                "Forward and reverse primer hits for " +\
                                elements[3][:-8] +\
                                " are on different chromosomes")
                        rev_start = int(elements[1])
                        rev_end = int(elements[2])
                        if fwd_start < rev_start:
                            start_pos_wprimers = str(fwd_start)
                            end_pos_wprimers = str(rev_end)
                            start_pos_noprimers = str(fwd_end)
                            end_pos_noprimers = str(rev_start)
                        else:
                            start_pos_wprimers = str(rev_start)
                            end_pos_wprimers = str(fwd_end)
                            start_pos_noprimers = str(rev_end)
                            end_pos_noprimers = str(fwd_start)
                        trg_coord_wprimers_f.write(
                            '\t'.join([
                                chrom,
                                start_pos_wprimers,
                                end_pos_wprimers,
                                target+'\n']))
                        trg_coord_noprimers_f.write(
                            '\t'.join([
                                chrom,
                                start_pos_noprimers,
                                end_pos_noprimers,
                                target+'\n']))
                        chrom = ""
                        target = ""

# Extract sequence captured by primers from reference
captured_seq = os.path.join(prim_map_dir, "captured_seq.fasta")
rule extract_captured_seq:
    input: 
        ref_genome_index, 
        ref = ref_genome, 
        bed = trg_coords_noprimers
    output: captured_seq
    shell: 
        "bedtools getfasta -fi {input.ref} -bed {input.bed} > {output}"
# Check mapping for issues
rule investigate_mapping:
    input:
        primers_tsv, 
        trg_coords_noprimers, 
        code = os.path.join("workflow", "notebooks", "investigate_mapping.Rmd")
    output: 
        multiext(notebooks["investigate_mapping"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Assess marker performance based on May 2022 sequencing run
may2022_readcount = os.path.join("resources", "readcount.txt")
samples_uci = os.path.join("resources", "Samples_GTseek_UCI_Feb2022_Pv.csv")
may2022_readcounts_metadata = os.path.join(
    "results", 
    "may2022_readcounts_metadata.csv")
rule collate_may2022_readcounts_metadata:
    input:
        read_counts = may2022_readcount, 
        metadata = samples_uci, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "collate_readcounts_metadata.R")
    output: may2022_readcounts_metadata
    shell: 
        "Rscript {input.code} --read_counts {input.read_counts} --metadata "
        "{input.metadata} --out {output}"
microhap_combined = os.path.join(
    "resources", 
    "Microhap_PV_combined-data_2023-06-21")
trgs2filter_dir = os.path.join("results", "trgs2filter")
trgs2filter_good_amp = os.path.join(trgs2filter_dir, "good_amp.csv")
rule assess_marker_performance:
    input:
        microhap_combined, 
        os.path.join("resources", "GBPCD_meta.csv"), 
        code = os.path.join("workflow", "notebooks", "marker_performance.Rmd")
    output: 
        multiext(notebooks["marker_performance"], ".nb.html", ".pdf", ".md"), 
        selected_trgs = trgs2filter_good_amp
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """
# Filter marker set to those that performed well. Note that there are a 
# number of targets that are in the SeekDeep ID file but do not show up 
# in the May 2022 data (aside from SNP markers and other targets 
# filtered out in previous steps). These targets are also filtered out 
# in this step, in addition to those that were in the May 2022 data but 
# didn't amplify well.
trg_coords_good_amp = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_good_amp.bed")
rule filter_bed:
    input: 
        inp_bed = trg_coords_noprimers, 
        targets_keep = trgs2filter_good_amp
    output: trg_coords_good_amp
    run:
        # Read in list of targets to keep
        with open(input.targets_keep, 'r') as targets_f:
            targets_keep = []
            for line in targets_f:
                targets_keep.append(line.rstrip())
        # Filter out targets that don't show up well in May 2022 data
        with open(input.inp_bed, 'r') as inp_bed:
            with open(output[0], 'w') as out_bed:
                for line in inp_bed:
                    elements = line.rstrip().split()
                    if elements[3] in targets_keep:
                        out_bed.write(line)


# Download MalariaGEN data
malariagen_dir = os.path.join("results", "MalariaGEN")
malariagen_orig_dir = os.path.join(malariagen_dir, "orig")
malariagen_metadata = os.path.join(malariagen_orig_dir, "Pv4_samples.txt")
rule dwnld_metadata:
    output: malariagen_metadata
    shell: 
        "wget -P {malariagen_orig_dir} "
        "https://www.malariagen.net/wp-content/uploads/2023/11/Pv4_samples.txt"
malariagen_coi = os.path.join(malariagen_orig_dir, "Pv4_fws.txt")
rule dwnld_coi:
    output: malariagen_coi
    shell: 
        "wget -P {malariagen_orig_dir} "
        "https://www.malariagen.net/wp-content/uploads/2023/11/Pv4_fws.txt"
chromosome_nums = list(map(lambda c: str(c).zfill(2),range(1, 15)))
# It would be more efficient to download all the files with one wget, 
# but I can't figure out how to tell Snakemake it can obtain all the 
# wildcard outputs with one execution of a rule
rule dwnld_genotypes:
    output:
        temp(os.path.join(malariagen_orig_dir,"Pv4_PvP01_{chrom}_v1.{ext}"))
    resources:
        time = 600, 
        mem_mb = 2000, 
        # Our cluster's data transfer node. Modify as necessary if 
        # running on a different machine.
        partition = "DTN"
    threads: 1
    benchmark: 
        os.path.join("benchmarks", "dwnld_genotypes_{chrom}_{ext}.tsv")
    shell: 
        "wget -P {malariagen_orig_dir} "
        "ftp://ngs.sanger.ac.uk/production/malaria/Resource/30/Pv4_vcf/"
        "Pv4_PvP01_{wildcards.chrom}_v1.{wildcards.ext}"

# Filter and concatenate MalariaGEN VCFs
malariagen_chromrenamed = "Pv4_PvP01_{chrom}_v1_chromrenamed.bcf"
# Rename chromosomes to match convention in reference genome
rule rename_chroms:
    input:
        chrom_key = os.path.join("resources", "chrom_key.tsv"), 
        vcf = os.path.join(malariagen_orig_dir, "Pv4_PvP01_{chrom}_v1.vcf.gz")
    output: temp(malariagen_chromrenamed)
    resources:
        time = 10, 
        mem_mb = 2000
    threads: 1
    benchmark: os.path.join("benchmarks", "rename_chroms_{chrom}.tsv")
    shell: 
        "bcftools annotate --rename-chrs {input.chrom_key} -O u -o {output} "
        "{input.vcf}"
malariagen_filteredbymarker = "Pv4_PvP01_{chrom}_v1_filteredbymarker.bcf"
# Index VCFs or BCFs
rule index_variant_file:
    input: "{variant_file}"
    output: temp("{variant_file}.csi")
    shell: "bcftools index {input}"
# Filter to genomic regions of interest
rule filter_bcf_by_marker:
    input:
        malariagen_chromrenamed + ".csi", 
        bed = trg_coords_good_amp, 
        vcf = malariagen_chromrenamed
    output: temp(malariagen_filteredbymarker)
    shell: "bcftools view -R {input.bed} -O u -o {output} {input.vcf}"
malariagen_prep_dir = os.path.join(malariagen_dir, "preprocessed")
malariagen_concat = os.path.join(malariagen_prep_dir, "concatenated.vcf.gz")
# Concatenate data from all chromosomes into one VCF
rule concat_bcfs:
    input: expand(malariagen_filteredbymarker, chrom=chromosome_nums)
    output: malariagen_concat
    shell: "bcftools concat -o {output} {input}"
# Filter out variants that failed the MalariaGEN QC process
malariagen_filtered_by_snpqc = os.path.join(
    malariagen_prep_dir, 
    "filtered_by_snpqc.vcf.gz")
rule filter_vcf_by_snpqc:
    input: malariagen_concat
    output: malariagen_filtered_by_snpqc
    shell:
        """
        bcftools view --include 'FILTER="PASS"' -o {output} {input}
        """
# Compute the proportion of missing variants for each sample. Note that 
# the reverse (proportion missing for each variant) is not addressed, 
# because the haplotype-based analyses that are performed downstream 
# are robust with respect to missing SNP calls.
malariagen_missbysamp = os.path.join(malariagen_prep_dir, "missbysamp.tsv")
rule compute_missbysamp:
    input: malariagen_filtered_by_snpqc
    output: malariagen_missbysamp
    shadow: "full"
    shell: "vcftools --gzvcf {input} --missing-indv --stdout > {output}"
filtered_sample_list = os.path.join("results", "filtered_samples.txt")
filtered_sample_metadata = os.path.join(
    malariagen_prep_dir, 
    "sample_metadata.csv")
unfiltered_sites = os.path.join("results", "unfiltered_sites.geojson")
checkpoint inspect_sample_properties:
    input:
        malariagen_metadata, 
        malariagen_coi, 
        malariagen_missbysamp, 
        code = os.path.join("workflow", "notebooks", "sample_properties.Rmd")
    output: 
        multiext(notebooks["sample_properties"], ".nb.html", ".pdf", ".md"), 
        samp_list = filtered_sample_list, 
        samp_metadata = filtered_sample_metadata, 
        sites = unfiltered_sites
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """
# Filter samples based on various factors investigated in the previous 
# step
malariagen_filtered_by_samp = os.path.join(
    malariagen_prep_dir, 
    "filtered_by_samp.vcf.gz")
rule filter_samps:
    input:
        vcf = malariagen_filtered_by_snpqc, 
        samp_list = filtered_sample_list
    output: malariagen_filtered_by_samp
    shell: "bcftools view -S {input.samp_list} -o {output} {input.vcf}"

# Get haplotype sequences by applying variants to reference genome
ref_w_variants = "{samp}_ref_w_variants.fa"
# Apply variants to unfiltered reference genome
rule apply_variants:
    input:
        malariagen_filtered_by_samp + ".csi", 
        vcf = malariagen_filtered_by_samp, 
        ref = ref_genome
    output: temp(ref_w_variants)
    shell:
        # Note that this method of obtaining the haplotype sequence 
        # ignores the read depth of each allele, which could be a 
        # limitation. However, the MalariaGEN authors have used a 
        # diploid VCF to encode situations where the read depths of the 
        # reference and alternate alleles are somewhat close as 
        # heterozygous calls (i.e., GT is 0/1). These calls are 
        # replaced with the appropriate IUPAC ambiguity code using the 
        # -I flag. Note that in this case this yields the same results 
        # as the -H I argument (I verified this).
        "bcftools consensus -s {wildcards.samp} -I -f {input.ref} {input.vcf} "
        "> {output}"
mg_microhap_fasta_dir = os.path.join(mg_microhap_dir, "fasta")
hapseq = os.path.join(mg_microhap_fasta_dir, "{samp}_hapseq.fa")
# Filter to the targets of interest
rule filter2hapseq:
    input: 
        fasta = ref_w_variants, 
        bed = trg_coords_good_amp
    output: hapseq
    # Index files for the input fasta are automatically generated
    shadow: "full"
    shell:
        "bedtools getfasta -fi {input.fasta} -bed {input.bed} -fo {output}"
# Finally, convert the data to tidy format and join metadata
mg_microhap_tidy = os.path.join(mg_microhap_dir, "mg_microhap.csv")
mg_microhap_aligned = os.path.join(mg_microhap_dir, "mg_microhap_aligned.rds")
rule tidy_microhap:
    input:
        lambda wildcards: expand_w_samp_list(wildcards, hapseq), 
        trg_coords = trg_coords_good_amp, 
        sample_metadata = filtered_sample_metadata, 
        code = os.path.join("workflow", "scripts", "tidy_microhap.R")
    output: 
        # Unfortunately there is no good alternative to specifying the 
        # alignment FASTAs as a directory. The target list depends on 
        # assess_marker_performance, and so cannot be expanded in 
        # advance, and I'm not aware of a way to tell Snakemake that 
        # one execution of a rule can create multiple wildcard outputs.
        directory(mg_microhap_aligned_fasta_dir), 
        aligned_haps = mg_microhap_aligned, 
        mh_csv = mg_microhap_tidy
    resources:
        time = 10, 
        mem_mb = 4000
    threads: 1
    benchmark: os.path.join("benchmarks", "tidy_microhap.tsv")
    shell: 
        # Snakemake does not create the aligned FASTA directory 
        # automatically. I gather this is by design when you use a 
        # directory as output - it expects the rule itself to create 
        # it.
        """
        mkdir {mg_microhap_aligned_fasta_dir}
        Rscript {input.code} --mh_fasta_dir {mg_microhap_fasta_dir} \
            --trg_coords {input.trg_coords} --sample_metadata \
            {input.sample_metadata} --aligned_haps {output.aligned_haps} \
            --out_csv {output.mh_csv} --alignments_fasta_dir \
            {mg_microhap_aligned_fasta_dir}
        """

# Marker and sample diversity
# Convert tibble of msa alignments to pegas haplotypes and add 
# haplotype IDs to the main CSV file
mg_microhap_haplo = os.path.join(mg_microhap_dir, "mg_microhap_haplo.rds")
mg_microhap_w_hap_ids = os.path.join(
    mg_microhap_dir, 
    "mg_microhap_w_hap_ids.csv")
rule alignment2hap:
    input:
        mh_aligned = mg_microhap_aligned, 
        mh_csv = mg_microhap_tidy, 
        code = os.path.join("workflow", "scripts", "alignment2hap.R")
    output: 
        mh_haplo = mg_microhap_haplo, 
        mh_csv_w_hap_ids = mg_microhap_w_hap_ids
    shell:
        "Rscript {input.code} --alignments {input.mh_aligned} --mh_csv "
        "{input.mh_csv} --haplotypes {output.mh_haplo} --mh_csv_w_hap_ids "
        "{output.mh_csv_w_hap_ids}"
mg_microhap_gd = os.path.join(mg_microhap_dir, "mg_microhap_gd.rds")
rule csv2genind:
    input:
        mh_csv = mg_microhap_w_hap_ids, 
        code = os.path.join("workflow", "scripts", "csv2genind.R")
    output: mg_microhap_gd
    shell: "Rscript {input.code} --mh_csv {input.mh_csv} --mh_gd {output}"
rule estimate_diversity:
    input:
        mg_microhap_haplo, 
        mg_microhap_gd, 
        mg_microhap_tidy, 
        code = os.path.join("workflow", "notebooks", "diversity.Rmd")
    output: 
        multiext(notebooks["diversity"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Linkage disequilibrium
ld_res = os.path.join("results", "ld_res.rds")
seed = 59404051
rule check_ld:
    input:
        mh_data = mg_microhap_gd, 
        code = os.path.join("workflow", "scripts", "check_ld.R")
    output: ld_res
    resources:
        time = 60, 
        mem_mb = 2000
    threads: 1
    benchmark: os.path.join("benchmarks", "check_ld.tsv")
    shell:
        "Rscript {input.code} --data_gd {input.mh_data} --seed {seed} --out "
        "{output}"
rule assess_ld:
    input:
        ld_res, 
        code = os.path.join("workflow", "notebooks", "ld.Rmd")
    output:
        multiext(notebooks["ld"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

# Selection
# Convert the msa alignment objects to ape DNAbin objects, subset to 
# have one for each MalariaGEN population
mh_popbin = os.path.join(mg_microhap_dir, "mg_microhap_popbin.rds")
rule alignment2popbin:
    input:
        mh_aligned = mg_microhap_aligned, 
        mh_csv = mg_microhap_tidy, 
        code = os.path.join("workflow", "scripts", "alignment2popbin.R")
    output: mh_popbin
    resources:
        time = 5, 
        mem_mb = 1000
    threads: 1
    benchmark: os.path.join("benchmarks", "alignment2popbin.tsv")
    shell:
        "Rscript {input.code} --alignments {input.mh_aligned} --mh_csv "
        "{input.mh_csv} --out {output}"
trgs2filter_selec = os.path.join(trgs2filter_dir, "selec.csv")
rule assess_selection:
    input:
        mh_popbin, 
        code = os.path.join("workflow", "notebooks", "selection.Rmd")
    output:
        multiext(notebooks["selection"], ".nb.html", ".pdf", ".md"), 
        trgs2filter_selec
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

# Filter data for selection
mg_microhap_filtered4popgen = os.path.join(
    mg_microhap_dir, 
    "mg_microhap_filtered4popgen.csv")
rule filter4popgen:
    input:
        mh_in = mg_microhap_w_hap_ids, 
        trgs2filter = trgs2filter_selec, 
        code = os.path.join("workflow", "scripts", "filter4popgen.R")
    output: mg_microhap_filtered4popgen
    shell:
        "Rscript {input.code} --mh_in {input.mh_in} --trgs2filter "
        "{input.trgs2filter} --mh_out {output}"

# IBD-based relatedness
rel_mat = os.path.join("results", "rel_mat.rds")
rule compute_rel:
    input:
        mh_data = mg_microhap_filtered4popgen, 
        code = os.path.join("workflow", "scripts", "compute_rel.R")
    resources: 
        time = 10, 
        mem_mb = 2000
    threads: 1
    benchmark: os.path.join("benchmarks", "compute_rel.tsv")
    output: rel_mat
    shell: 
        "Rscript {input.code} --gen_data {input.mh_data} --seed {seed} "
        "--rel_mat {output}"
mg_pop_key = os.path.join("resources", "mg_pop_key.csv")
sample_rel = os.path.join("results", "sample_rel.csv")
rule analyze_relatedness:
    input:
        rel_mat, 
        mg_microhap_filtered4popgen, 
        mg_pop_key, 
        code = os.path.join("workflow", "notebooks", "relatedness.Rmd")
    output:
        multiext(notebooks["relatedness"], ".nb.html", ".pdf", ".md"), 
        sample_rel
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

# Make figures
rule plot_panel_marker_metrics:
    input:
        window_stats = os.path.join("resources", "filtered_windows_tab.txt"), 
        targets = trgs2filter_good_amp, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "panel_marker_metrics_plot.R")
    output: 
        multiext(os.path.join(figdir,figs["panel_marker_metrics"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --window_stats {input.window_stats} --targets "
        "{input.targets} --out_base {params.basename}"
may2022_total_readcount = os.path.join(
    "resources", 
    "may2022_totalreadcount.txt")
rule plot_otreads_by_protocol:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        total_readcounts = may2022_total_readcount, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "otreads_by_protocol_barplots.R")
    output: 
        multiext(os.path.join(figdir,figs["otreads_by_protocol"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --total_read_counts "
        "{input.total_readcounts} --out_base {params.basename}"
rule plot_marker_reads_by_protocol:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        selected_trgs = trgs2filter_good_amp, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "marker_reads_by_protocol_boxplots.R")
    output: 
        multiext(os.path.join(figdir,figs["marker_reads_by_protocol"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --selected_trgs {input.selected_trgs} "
        "--out_base {params.basename}"
rule plot_otreads_vs_density:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        total_readcounts = may2022_total_readcount, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "otreads_vs_density_scatterplots.R")
    output: 
        multiext(os.path.join(figdir,figs["otreads_vs_density"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --total_read_counts "
        "{input.total_readcounts} --out_base {params.basename}"

# Copy relevant outputs to shared Google Drive
cloud_dir = "UNCC_GDrive:Alfred-Liz-PV-microhaplotype-files"
rule push_results_notebooks:
    input: os.path.join(rendered_notebook_dir, "{nb}.nb.html")
    output: touch(nb_flag_template)
    priority: -1
    shell: "rclone copy {input} {cloud_dir}/notebooks --drive-shared-with-me"
rule push_figs:
    input: os.path.join(figdir, "{fig}.pdf")
    output: touch(fig_flag_template)
    priority: -1
    shell: "rclone copy {input} {cloud_dir}/figs --drive-shared-with-me"

# Delete all outputs in preparation for rerunning pipeline from nothing
rule clean:
    shell: "rm -rf results benchmarks logs .flags"
