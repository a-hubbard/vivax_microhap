import glob
import os


def expand_w_samp_list(wildcards, expression):
    """Read in list of samples and use to expand provided expression.

    This function reads the output of inspect_sample_properties to get 
    the list of samples selected for analysis. It then uses this list 
    to expand the provided wildcard expression.

    Arguments:
    wildcards -- Snakemake wildcards object
    expression -- Either a string or a list containing one or more 
        wildcard expressions containing the wildcard "samp"
    """
    samp_txt = checkpoints.inspect_sample_properties.get(**wildcards).output.\
        samp_list
    samp_l = []
    with open(samp_txt, 'r') as samp_f:
        for line in samp_f:
            samp_l.append(line.rstrip())
    return(expand(expression,samp=samp_l))


# Identify rules to be executed locally
localrules:
    refmt_primers, 
    dwnld_ref, 
    unzip_ref, 
    index_ref_bwa, 
    primers2fastq, 
    find_sa_coords, 
    map_primers, 
    sam2bam, 
    bam2bed, 
    captured_seq_bounds, 
    extract_captured_seq, 
    investigate_mapping, 
    collate_may2022_readcounts_metadata, 
    assess_marker_performance, 
    index_ref_samtools, 
    targets_bed2gff, 
    analyze_ampseqc_results, 
    filter_bed, 
    filter_primers, 
    multiqc_uci1223_data, 
    list_uci1223_fastqs, 
    postproc_dada2_uci1223, 
    asv2cigar_uci1223, 
    assess_microhap_quantity_uci1223, 
    dwnld_ref_gff, 
    unzip_ref_gff, 
    subset_annot, 
    refmt_annot, 
    dwnld_metadata, 
    dwnld_coi, 
    index_variant_file, 
    filter_bcf_by_locus, 
    concat_bcfs, 
    filter_vcf_by_snpqc, 
    compute_missbysamp, 
    inspect_sample_properties, 
    filter_samps, 
    apply_variants, 
    filter2hapseq, 
    alignment2hap, 
    csv2genind, 
    estimate_diversity, 
    assess_ld, 
    assess_selection, 
    filter4popgen, 
    analyze_relatedness, 
    plot_panel_target_metrics, 
    plot_otreads_by_protocol, 
    plot_locus_reads_by_protocol, 
    plot_otreads_vs_density, 
    plot_reads_by_locus, 
    plot_mean_het_by_pop, 
    plot_sample_rel, 
    rel_heatmaps, 
    map_rel_net, 
    compile_figs_tables, 
    push_results_notebooks, 
    push_figs, 
    push_figs_tables_doc, 
    clean

# Notebooks
rendered_notebook_dir = os.path.join("results", "notebooks")
notebooks = {}
notebook_basenames = [
    "investigate_mapping", 
    "marker_performance", 
    "ampseqc_results", 
    "microhap_quantity_uci1223", 
    "sample_properties", 
    "diversity", 
    "ld", 
    "selection", 
    "relatedness"]
for nb in notebook_basenames:
    notebooks[nb] = os.path.join(rendered_notebook_dir, nb)
nb_flag_template = os.path.join(".flags", "push_nb_{nb}")
# Figures
# All figures, even supplemental ones, are defined here, in the same 
# fashion, to facilitate easy swapping of inserts between supplemental 
# material and the main document
figdir = os.path.join("results", "figs")
figs = {
    "panel_target_metrics": "fig1", 
    "otreads_by_protocol": "fig2", 
    "locus_reads_by_protocol": "fig3", 
    "otreads_vs_density": "fig4", 
    "reads_by_locus": "suppfig1", 
    "mean_het_by_pop": "suppfig2", 
    "rel_hist": "suppfig3", 
    "rel_heatmaps_country": "fig5", 
    "rel_heatmaps_site": "suppfig4", 
    "site_rel_net": "fig6"}
fig_flag_template = os.path.join(".flags", "push_fig_{fig}")
# Documents
figs_tables_doc = os.path.join("results", "all_figs_tables.pdf")
figs_tables_doc_flag = os.path.join(".flags", "push_figs_tables_doc")
# Other outputs
locus_metadata = os.path.join("results", "locus_metadata.tsv")
microhap_dir = os.path.join("results", "microhap")
mg_microhap_dir = os.path.join(microhap_dir, "MalariaGEN")
mg_microhap_aligned_fasta_dir = os.path.join(mg_microhap_dir, "aligned_fastas")
rule all:
    input:
        expand("{nb}.{ext}", nb=list(notebooks.values()), ext=["pdf","md"]), 
        expand(nb_flag_template, nb=notebook_basenames), 
        expand(os.path.join(figdir,"{fig}.png"), fig=list(figs.values())), 
        expand(fig_flag_template, fig=list(figs.values())), 
        figs_tables_doc_flag, 
        locus_metadata, 
        mg_microhap_aligned_fasta_dir
# For others attempting to run the pipeline on their own machine
rule replicate:
    input:
        expand(
            "{nb}.{ext}", 
            nb=list(notebooks.values()), 
            ext=["nb.html","pdf","md"]), 
        expand(
            os.path.join(figdir,"{fig}.{ext}"), 
            fig=figs, 
            ext=["png","pdf"]), 
        figs_tables_doc, 
        locus_metadata

# Reformat and filter primers
sd_id_file = os.path.join("resources", "id_file.txt")
primer_info = os.path.join(
    "resources", 
    "Microhaplotype primers_fixedRCpf_2023-03-02.xlsx")
primers_tsv = os.path.join("results", "primers.tsv")
fwd_primers = os.path.join("results", "fwd_primers.fasta")
rev_primers = os.path.join("results", "rev_primers.fasta")
rule refmt_primers:
    input:
        sd_id_file = sd_id_file, 
        primer_info = primer_info, 
        code = os.path.join("workflow", "scripts", "refmt_primers.R")
    output:
        primers_tsv = primers_tsv, 
        fwd_primers = fwd_primers, 
        rev_primers = rev_primers
    shell:
        "Rscript {input.code} --sd_id_file {input.sd_id_file} --primer_info "
        "'{input.primer_info}' --primers_tsv {output.primers_tsv} "
        "--fwd_primers {output.fwd_primers} --rev_primers {output.rev_primers}"

# Get primer locations and target reference sequences
# Download reference genome
ref_genome_zip = os.path.join("results", "GCA_900093555.2.zip")
rule dwnld_ref:
    output: temp(ref_genome_zip)
    shell: 
        "datasets download genome accession GCA_900093555.2 --filename {output}"
ref_genome_dir = os.path.join("results", "ref_genome")
ref_genome = os.path.join(
    ref_genome_dir, 
    "GCA_900093555.2", 
    "GCA_900093555.2_GCA_900093555_genomic.fna")
rule unzip_ref:
    input: ref_genome_zip
    output: 
        ref_genome, 
        os.path.join(ref_genome_dir, "README.md"), 
        os.path.join(ref_genome_dir, "assembly_data_report.jsonl"), 
        os.path.join(ref_genome_dir, "dataset_catalog.json")
    shell:
        """
        unzip {input} -d {ref_genome_dir}
        mv -f {ref_genome_dir}/ncbi_dataset/data/* {ref_genome_dir}/
        rm -r {ref_genome_dir}/ncbi_dataset
        """
# Necessary for mapping
ref_genome_index = multiext(ref_genome, ".amb", ".ann", ".bwt", ".pac", ".sa")
rule index_ref_bwa:
    input: ref_genome
    output: ref_genome_index
    shell: "bwa index {input}"
# Create FASTQ with primer sequences
prim_map_dir = os.path.join("results", "primer_mapping")
primers_fq = os.path.join(prim_map_dir, "primers.fastq")
rule primers2fastq:
    input: primers_tsv
    output: primers_fq
    run:
        with open(input[0], 'r') as primer_list:
            with open(output[0], 'w') as out_fq:
                # Skip header
                primer_list.readline()
                for line in primer_list:
                    elements = line.rstrip().split()
                    target = elements[0]
                    fwd_primer = elements[1]
                    rev_primer = elements[2]
                    out_fq.write("@" + target + "_forward\n")
                    out_fq.write(fwd_primer + '\n')
                    out_fq.write("+\n")
                    out_fq.write("~" * len(fwd_primer) + '\n')
                    out_fq.write("@" + target + "_reverse\n")
                    out_fq.write(rev_primer + '\n')
                    out_fq.write("+\n")
                    out_fq.write("~" * len(rev_primer) + '\n')
# In the next two steps, aln and samse are used to employ the BWA-
# backtrack algorithm, which is meant for sequences less than 100 bp, 
# and especially less than 70 bp
sa_coords = os.path.join(prim_map_dir, "mapping.sai")
rule find_sa_coords:
    input: 
        ref_genome_index, 
        ref = ref_genome, 
        query = primers_fq
    output: sa_coords
    shell: "bwa aln {input.ref} {input.query} > {output}"
primer_mapping_sam = sa_coords[:-3] + "sam"
rule map_primers:
    input:
        ref_genome_index, 
        ref = ref_genome, 
        query = primers_fq, 
        sa_coords = sa_coords
    output: primer_mapping_sam
    shell: "bwa samse {input.ref} {input.sa_coords} {input.query} > {output}"
rule sam2bam:
    input: "{filename}.sam"
    output: temp("{filename}.bam")
    shell: "samtools view -h -b {input} > {output}"
rule bam2bed:
    input: "{filename}.bam"
    output: "{filename}.bed"
    shell: "bedtools bamtobed -i {input} > {output}"
# Convert primer coordinates to target coordinates, both with and 
# without primers
primer_mapping_bed = primer_mapping_sam[:-3] + "bed"
trg_coords_wprimers = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_wprimers.bed")
trg_coords_noprimers = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_noprimers.bed")
rule captured_seq_bounds:
    input: primer_mapping_bed
    output: 
        coords_wprimers = trg_coords_wprimers, 
        coords_noprimers = trg_coords_noprimers
    run:
        with open(input[0], 'r') as primer_coord_f:
            with open(output.coords_wprimers, 'w') as trg_coord_wprimers_f, \
                    open(output.coords_noprimers, 'w') as trg_coord_noprimers_f:
                # The following logic assumes the bed file is sorted 
                # such that forward primers are before reverse primers 
                # and all hits for a given primer pair are grouped 
                # together. Due to the small size of the bed file in 
                # question, this assumption was only verified by 
                # visual inspection.
                chrom = ""
                target = ""
                for line in primer_coord_f:
                    elements = line.rstrip().split()
                    if elements[3].split('_')[-1] == "forward":
                        if elements[3][:-8] == target:
                            raise ValueError(
                                "Forward primer for " + elements[3][:-8] +\
                                " is a duplicate")
                        chrom = elements[0]
                        target = elements[3][:-8]
                        fwd_start = int(elements[1])
                        fwd_end = int(elements[2])
                    if elements[3].split('_')[-1] == "reverse":
                        if elements[3][:-8] != target:
                            raise ValueError(
                                "Reverse primer for " + elements[3][:-8] +\
                                " does not match previous forward primer, " +\
                                "or is a duplicate")
                        elif elements[0] != chrom:
                            raise ValueError(
                                "Forward and reverse primer hits for " +\
                                elements[3][:-8] +\
                                " are on different chromosomes")
                        rev_start = int(elements[1])
                        rev_end = int(elements[2])
                        if fwd_start < rev_start:
                            start_pos_wprimers = str(fwd_start)
                            end_pos_wprimers = str(rev_end)
                            start_pos_noprimers = str(fwd_end)
                            end_pos_noprimers = str(rev_start)
                        else:
                            start_pos_wprimers = str(rev_start)
                            end_pos_wprimers = str(fwd_end)
                            start_pos_noprimers = str(rev_end)
                            end_pos_noprimers = str(fwd_start)
                        trg_coord_wprimers_f.write(
                            '\t'.join([
                                chrom,
                                start_pos_wprimers,
                                end_pos_wprimers,
                                target+'\n']))
                        trg_coord_noprimers_f.write(
                            '\t'.join([
                                chrom,
                                start_pos_noprimers,
                                end_pos_noprimers,
                                target+'\n']))
                        chrom = ""
                        target = ""

# Check mapping for issues
rule investigate_mapping:
    input:
        primers_tsv, 
        trg_coords_noprimers, 
        code = os.path.join("workflow", "notebooks", "investigate_mapping.Rmd")
    output: 
        multiext(notebooks["investigate_mapping"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Assess marker performance based on May 2022 sequencing run
may2022_readcount = os.path.join("resources", "readcount.txt")
samples_uci = os.path.join("resources", "Samples_GTseek_UCI_Feb2022_Pv.csv")
may2022_readcounts_metadata = os.path.join(
    "results", 
    "may2022_readcounts_metadata.csv")
rule collate_may2022_readcounts_metadata:
    input:
        read_counts = may2022_readcount, 
        metadata = samples_uci, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "collate_readcounts_metadata.R")
    output: may2022_readcounts_metadata
    shell: 
        "Rscript {input.code} --read_counts {input.read_counts} --metadata "
        "{input.metadata} --out {output}"
microhap_combined = os.path.join(
    "resources", 
    "Microhap_PV_combined-data_2023-06-21")
rule assess_marker_performance:
    input:
        microhap_combined, 
        os.path.join("resources", "GBPCD_meta.csv"), 
        code = os.path.join("workflow", "notebooks", "marker_performance.Rmd")
    output: 
        multiext(notebooks["marker_performance"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Check quality of UCI 12/23 data
raw_seq_dir = os.path.join("resources", "seq_data")
uci1223_dir = os.path.join(raw_seq_dir, "lbradle123121251")
uci1223_seqs = [
    f for f in glob.glob(os.path.join(uci1223_dir,"*.txt.gz")) if "PrNotRecog" 
    not in f]
qc_dir = os.path.join("results", "seq_qc")
uci1223_qc_dir = os.path.join(qc_dir, "uci1223")
uci1223_fastqc_dir = os.path.join(uci1223_qc_dir, "fastqc")
uci1223_fastqc_outbase = [
    os.path.join(uci1223_fastqc_dir,os.path.basename(fq)[:-7]+"_fastqc.{ext}") 
    for fq in uci1223_seqs]
uci1223_fastqc_out = expand(uci1223_fastqc_outbase, ext=["html","zip"])
rule fastqc_uci1223_data:
    input: uci1223_seqs
    output: uci1223_fastqc_out
    resources:
        time = 60, 
        mem_mb = 8000
    threads: 8
    benchmark: os.path.join("benchmarks", "fastqc_uci1223_data.tsv")
    shell: "fastqc -o {uci1223_fastqc_dir} -t {threads} {input}"
uci1223_multiqc_dir = os.path.join(uci1223_qc_dir, "multiqc")
uci1223_multiqc_report = os.path.join(
    uci1223_multiqc_dir, 
    "multiqc_report.html")
rule multiqc_uci1223_data:
    input: uci1223_fastqc_out
    output: uci1223_multiqc_report
    shell: "multiqc --outdir {uci1223_multiqc_dir} --force {uci1223_fastqc_dir}"

# Run AmpSeQC on May 2022 and UCI 12/23 data
# AmpSeQC requires a FAI index of the reference genome
ref_genome_fai = ref_genome + ".fai"
rule index_ref_samtools:
    input: ref_genome
    output: ref_genome_fai
    shell: "samtools faidx {input}"
# AmpSeQC requires the targets to be formatted as GFF
trg_annot = os.path.join(prim_map_dir, "targets.gff")
rule targets_bed2gff:
    input: trg_coords_wprimers
    output: trg_annot
    run:
        with open(input[0], 'r') as bed_f:
            with open(output[0], 'w') as gff_f:
                gff_f.write("##gff-version 3\n")
                for line in bed_f:
                    elements = line.rstrip().split()
                    chrom = elements[0]
                    start_pos = str(int(elements[1]) + 1)
                    end_pos = elements[2]
                    target = elements[3]
                    gff_f.write(
                        '\t'.join([
                            chrom,
                            "Pv_GTSeq",
                            "amplicon",
                            start_pos,
                            end_pos,
                            '.',
                            '+',
                            '.',
                            "ID="+target+'\n']))
# Path to AmpSeQC script
ampseqc = "~/AmpSeQC/AmpSeQC.py"
ampseqc_dir = os.path.join("results", "AmpSeQC")
ampseqc_seqrun_dir = os.path.join(ampseqc_dir, "{seqrun}")
read_counts = os.path.join(ampseqc_seqrun_dir, "read_counts.tsv")
may2022_seqs = glob.glob("resources/seq_data/PV_GTSeq_May2022/*.fastq.gz")
# Sort list so that READ2 will always follow READ1
uci1223_seqs.sort()
seq_dict = {"May2022": may2022_seqs, "UCI1223": uci1223_seqs}
rule run_ampseqc:
    input:
        ref_genome_fai, 
        targets = trg_annot, 
        fastqs = lambda wildcards: seq_dict[wildcards.seqrun], 
        ref = ref_genome
    params:
        results_dir = ampseqc_seqrun_dir, 
        fastq_args = lambda wildcards, input: 
            "../../../" + " ../../../".join(input.fastqs)
    # None of the sample-specific outputs are defined here because 
    # these don't seem to be guaranteed to be produced for every sample
    output:
        read_counts = read_counts
    resources:
        time = 120, 
        mem_mb = 8000
    threads: 2
    benchmark: os.path.join("benchmarks", "run_ampseqc_{seqrun}.tsv")
    conda: "ampseqc"
    shell:
        # To get AmpSeQC to run properly, I had to add the arguments 
        # -a AGATCGGAAGAGC -a2 GATCGTCGGACTG to the Trim Galore step. 
        # These are the first 13 bases of the reverse complements of 
        # our reverse and forward adapters, respectively. Although this 
        # behavior does not make sense to me, it appears that Trim 
        # Galore checks both R1 and R2 for AGATCGGAAGAGC, leading it to 
        # only trim the R1 reads, leading to a length mismatch between 
        # R1 and R2, which in turn causes them to be considered 
        # "improper" pairs and get filtered out by samtools. Manually 
        # specifying the adapters resolves this issue.
        """
        cd {params.results_dir}
        python {ampseqc} -c ../../../{output.read_counts} -r \
            ../../../{input.ref} -a ../../../{input.targets} --no_fastqc -p \
            {threads} {params.fastq_args}
        """
# Analyze AmpSeQC results to identify loci that amplified successfully
loci2filter_dir = os.path.join("results", "loci2filter")
loci2filter_good_amp = os.path.join(loci2filter_dir, "good_amp.csv")
rule analyze_ampseqc_results:
    input:
        expand(read_counts, seqrun=["May2022","UCI1223"]), 
        code = os.path.join("workflow", "notebooks", "ampseqc_results.Rmd")
    output: 
        multiext(notebooks["ampseqc_results"], ".nb.html", ".pdf", ".md"), 
        loci2filter_good_amp
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Filter target set to those present in sequencing runs
trg_coords_good_amp = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_good_amp.bed")
rule filter_bed:
    input:
        inp_bed = trg_coords_noprimers, 
        targets_keep = loci2filter_good_amp
    output: trg_coords_good_amp
    run:
        # Read in list of targets to keep
        with open(input.targets_keep, 'r') as targets_f:
            targets_keep = []
            for line in targets_f:
                targets_keep.append(line.rstrip())
        # Filter out targets not present in sequencing runs
        with open(input.inp_bed, 'r') as inp_bed:
            with open(output[0], 'w') as out_bed:
                for line in inp_bed:
                    elements = line.rstrip().split()
                    if elements[3] in targets_keep:
                        out_bed.write(line)
# Extract sequence captured by filtered list of targets
captured_seq = os.path.join(prim_map_dir, "captured_seq.fasta")
rule extract_captured_seq:
    input: 
        ref_genome_index, 
        ref = ref_genome, 
        bed = trg_coords_good_amp
    output: captured_seq
    shell: 
        "bedtools getfasta -fi {input.ref} -bed {input.bed} > {output}"
rule filter_primers:
    input:
        primers = os.path.join("results", "{direc}_primers.fasta"), 
        targets_keep = loci2filter_good_amp
    output: os.path.join("results", "{direc}_primers_filtered.fasta")
    run:
        # Read in list of targets to keep
        targets_keep = []
        with open(input.targets_keep, 'r') as targets_f:
            for line in targets_f:
                targets_keep.append(line.rstrip())
        # Filter FASTA of primers
        with open(input.primers, 'r') as primers_in:
            with open(output[0], 'w') as primers_out:
                keep_target = False
                for line in primers_in:
                    if line[0] == '>':
                        if line[1:].rstrip() in targets_keep:
                            keep_target = True
                            primers_out.write(line)
                    else:
                        if keep_target:
                            primers_out.write(line)
                            keep_target = False

# Preprocess UCI 12/23 data into microhaplotypes
# Create TSV file listing FASTQs
ampseq_dir = os.path.join("results", "AmpSeq")
ampseq_uci1223_dir = os.path.join(ampseq_dir, "uci1223")
uci1223_fastq_list = os.path.join(ampseq_uci1223_dir, "fastq_list.tsv")
rule list_uci1223_fastqs:
    input: uci1223_seqs
    output: uci1223_fastq_list
    run:
        with open(output[0], 'w') as out_f:
            for fq in input:
                fq_elements = os.path.basename(fq).split('-')
                if fq_elements[5] == "READ1":
                    sample_id = fq_elements[2]
                    r2_fq = fq.replace('READ1', 'READ2')
                    out_f.write('\t'.join([sample_id,fq,r2_fq+'\n']))
fwd_primers_filtered = os.path.join("results", "fwd_primers_filtered.fasta")
rev_primers_filtered = os.path.join("results", "rev_primers_filtered.fasta")
ampseq_tool_dir = "~/malaria-amplicon-pipeline"
uci1223_seqtab = os.path.join(ampseq_uci1223_dir, "run_dada2", "seqtab.tsv")
uci1223_bimeras = os.path.join(
    ampseq_uci1223_dir, 
    "run_dada2", 
    "ASVBimeras.txt")
uci1223_reads_summ = os.path.join(
    ampseq_uci1223_dir, 
    "run_dada2", 
    "reads_summary.txt")
rule run_ampseq_uci1223:
    input:
        fastq_list = uci1223_fastq_list, 
        fwd_primers = fwd_primers_filtered, 
        rev_primers = rev_primers_filtered
    output:
        uci1223_seqtab, 
        uci1223_bimeras, 
        uci1223_reads_summ
    resources:
        time = 120, 
        mem_mb = 128000
    threads: 16
    benchmark: os.path.join("benchmarks", "run_ampseq_uci1223.tsv")
    conda: "ampseq"
    shell:
        # Due to issues with the argument passing from 
        # AmpliconPipeline.py to runDADA2.R, it is necessary to specify 
        # some of the DADA2 parameters even though we are just using 
        # the pipeline defaults. Also, I think the Neafsey group used a 
        # trimRight of 2, but I'm not sure because they describe this 
        # as applying to the 5' end in the documentation of their 
        # scripts, despite the supplemental methods in Laverriere et 
        # al. (2022) and the DADA2 documentation suggesting this would 
        # be the 3' end (as well as common sense - it's called 
        # trimRight...).
        #
        # To get this tool to run properly, I had to add the arguments 
        # -a AGATCGGAAGAGC -a2 GATCGTCGGACTG to the Trim Galore step. 
        # These are the first 13 bases of the reverse complements of 
        # our reverse and forward adapters, respectively. Although this 
        # behavior does not make sense to me, it appears that Trim 
        # Galore checks both R1 and R2 for AGATCGGAAGAGC, leading it to 
        # only trim the R1 reads, leading to a length mismatch between 
        # R1 and R2, which in turn causes them to be considered 
        # "improper" pairs and get filtered out by samtools. Manually 
        # specifying the adapters resolves this issue.
        "python {ampseq_tool_dir}/AmpliconPipeline.py --path_to_meta "
        "{input.fastq_list} --pr1 {input.fwd_primers} --pr2 "
        "{input.rev_primers} --Class parasite --trimRight '2,2' --minLen 30 "
        "--max_consist 10 --omegaA '1e-120' --justConcatenate 0"
uci1223_asvtab = os.path.join(ampseq_uci1223_dir, "asvtab.tsv")
uci1223_asv_fastas = os.path.join(ampseq_uci1223_dir, "ASVSeqs.fasta")
rule postproc_dada2_uci1223:
    input:
        seqtab = uci1223_seqtab, 
        bimeras = uci1223_bimeras, 
        ref = captured_seq
    output:
        uci1223_asv_fastas, 
        asvtab = uci1223_asvtab
    conda: "ampseq"
    shell:
        "Rscript {ampseq_tool_dir}/postProc_dada2.R -s {input.seqtab} -ref "
        "{input.ref} -b {input.bimeras} -o {output.asvtab} --fasta "
        "--indel_filter 0.895 --strain PvP01"
asv2cigar_uci1223_dir = os.path.join(ampseq_uci1223_dir, "asv2cigar")
uci1223_seqtab_cigar = os.path.join(asv2cigar_uci1223_dir, "seqtab_cigar.tsv")
uci1223_asv2cigar_tab = os.path.join(asv2cigar_uci1223_dir, "asv2cigar_tab.tsv")
rule asv2cigar_uci1223:
    input:
        asv_fastas = uci1223_asv_fastas, 
        asvtab = uci1223_asvtab, 
        seqtab = uci1223_seqtab, 
        ref = captured_seq
    output:
        seqtab_cigar = uci1223_seqtab_cigar, 
        asv2cigar_tab = uci1223_asv2cigar_tab
    params:
        alignments_dir = os.path.join(asv2cigar_uci1223_dir, "alignments")
    conda: "ampseq"
    shell:
        "python {ampseq_tool_dir}/ASV_to_CIGAR.py --asv_to_cigar "
        "{output.asv2cigar_tab} --alignments {params.alignments_dir} --amp_db "
        "{input.ref} --min_reads 10 --exclude_bimeras --max_snv_dist 50 "
        "--max_indel_dist 15 {input.asv_fastas} {input.asvtab} {input.seqtab} "
        "{output.seqtab_cigar}"

# Assess amplicon yield from UCI 12/23 run
rule assess_microhap_quantity_uci1223:
    input:
        uci1223_reads_summ, 
        uci1223_seqtab_cigar, 
        code = os.path.join(
            "workflow", 
            "notebooks", 
            "microhap_quantity_uci1223.Rmd")
    output: 
        multiext(
            notebooks["microhap_quantity_uci1223"], 
            ".nb.html", 
            ".pdf", 
            ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Get gene symbols and names for panel
ref_gff_zip = os.path.join("results", "GCA_900093555.2_gff.zip")
rule dwnld_ref_gff:
    output: temp(ref_gff_zip)
    shell: 
        "datasets download genome accession GCA_900093555.2 --include gff3 "
        "--filename {output}"
ref_gff_dir = os.path.join("results", "ref_gff")
ref_gff = os.path.join(ref_gff_dir, "GCA_900093555.2", "genomic.gff")
rule unzip_ref_gff:
    input: ref_gff_zip
    output: 
        ref_gff, 
        os.path.join(ref_gff_dir, "README.md"), 
        os.path.join(ref_gff_dir, "assembly_data_report.jsonl"), 
        os.path.join(ref_gff_dir, "dataset_catalog.json")
    shell:
        """
        unzip {input} -d {ref_gff_dir}
        mv {ref_gff_dir}/ncbi_dataset/data/* {ref_gff_dir}/
        rm -r {ref_gff_dir}/ncbi_dataset
        """
panel_annot = os.path.join("results", "panel_annot.tsv")
rule subset_annot:
    input:
        ref_gff = ref_gff, 
        panel_bed = trg_coords_good_amp
    output: panel_annot
    shell: 
        "bedtools intersect -a {input.panel_bed} -b {input.ref_gff} -loj > "
        "{output}"
rule refmt_annot:
    input:
        panel_annot = panel_annot, 
        code = os.path.join("workflow", "scripts", "fmt_locus_metadata.R")
    output: locus_metadata
    shell: "Rscript {input.code} --annot {input.panel_annot} --out {output}"

# Download MalariaGEN data
malariagen_dir = os.path.join("results", "MalariaGEN")
malariagen_orig_dir = os.path.join(malariagen_dir, "orig")
malariagen_metadata = os.path.join(malariagen_orig_dir, "Pv4_samples.txt")
rule dwnld_metadata:
    output: malariagen_metadata
    shell: 
        "wget -P {malariagen_orig_dir} "
        "https://www.malariagen.net/wp-content/uploads/2023/11/Pv4_samples.txt"
malariagen_coi = os.path.join(malariagen_orig_dir, "Pv4_fws.txt")
rule dwnld_coi:
    output: malariagen_coi
    shell: 
        "wget -P {malariagen_orig_dir} "
        "https://www.malariagen.net/wp-content/uploads/2023/11/Pv4_fws.txt"
chromosome_nums = list(map(lambda c: str(c).zfill(2),range(1, 15)))
# It would be more efficient to download all the files with one wget, 
# but I can't figure out how to tell Snakemake it can obtain all the 
# wildcard outputs with one execution of a rule
rule dwnld_genotypes:
    output:
        temp(os.path.join(malariagen_orig_dir,"Pv4_PvP01_{chrom}_v1.{ext}"))
    resources:
        time = 600, 
        mem_mb = 2000, 
        # Our cluster's data transfer node. Modify as necessary if 
        # running on a different machine.
        partition = "DTN"
    threads: 1
    benchmark: 
        os.path.join("benchmarks", "dwnld_genotypes_{chrom}_{ext}.tsv")
    shell: 
        "wget -P {malariagen_orig_dir} "
        "ftp://ngs.sanger.ac.uk/production/malaria/Resource/30/Pv4_vcf/"
        "Pv4_PvP01_{wildcards.chrom}_v1.{wildcards.ext}"

# Filter and concatenate MalariaGEN VCFs
malariagen_chromrenamed = "Pv4_PvP01_{chrom}_v1_chromrenamed.bcf"
# Rename chromosomes to match convention in reference genome
rule rename_chroms:
    input:
        chrom_key = os.path.join("resources", "chrom_key.tsv"), 
        vcf = os.path.join(malariagen_orig_dir, "Pv4_PvP01_{chrom}_v1.vcf.gz")
    output: temp(malariagen_chromrenamed)
    resources:
        time = 10, 
        mem_mb = 2000
    threads: 1
    benchmark: os.path.join("benchmarks", "rename_chroms_{chrom}.tsv")
    shell: 
        "bcftools annotate --rename-chrs {input.chrom_key} -O u -o {output} "
        "{input.vcf}"
malariagen_filteredbylocus = "Pv4_PvP01_{chrom}_v1_filteredbylocus.bcf"
# Index VCFs or BCFs
rule index_variant_file:
    input: "{variant_file}"
    output: temp("{variant_file}.csi")
    shell: "bcftools index {input}"
# Filter to genomic regions of interest
rule filter_bcf_by_locus:
    input:
        malariagen_chromrenamed + ".csi", 
        bed = trg_coords_good_amp, 
        vcf = malariagen_chromrenamed
    output: temp(malariagen_filteredbylocus)
    shell: "bcftools view -R {input.bed} -O u -o {output} {input.vcf}"
malariagen_prep_dir = os.path.join(malariagen_dir, "preprocessed")
malariagen_concat = os.path.join(malariagen_prep_dir, "concatenated.vcf.gz")
# Concatenate data from all chromosomes into one VCF
rule concat_bcfs:
    input: expand(malariagen_filteredbylocus, chrom=chromosome_nums)
    output: malariagen_concat
    shell: "bcftools concat -o {output} {input}"
# Filter out variants that failed the MalariaGEN QC process
malariagen_filtered_by_snpqc = os.path.join(
    malariagen_prep_dir, 
    "filtered_by_snpqc.vcf.gz")
rule filter_vcf_by_snpqc:
    input: malariagen_concat
    output: malariagen_filtered_by_snpqc
    shell:
        """
        bcftools view --include 'FILTER="PASS"' -o {output} {input}
        """
# Compute the proportion of missing variants for each sample. Note that 
# the reverse (proportion missing for each variant) is not addressed, 
# because the haplotype-based analyses that are performed downstream 
# are robust with respect to missing SNP calls.
malariagen_missbysamp = os.path.join(malariagen_prep_dir, "missbysamp.tsv")
rule compute_missbysamp:
    input: malariagen_filtered_by_snpqc
    output: malariagen_missbysamp
    shadow: "full"
    shell: "vcftools --gzvcf {input} --missing-indv --stdout > {output}"
filtered_sample_list = os.path.join("results", "filtered_samples.txt")
filtered_sample_metadata = os.path.join(
    malariagen_prep_dir, 
    "sample_metadata.csv")
unfiltered_sites = os.path.join("results", "unfiltered_sites.geojson")
checkpoint inspect_sample_properties:
    input:
        malariagen_metadata, 
        malariagen_coi, 
        malariagen_missbysamp, 
        code = os.path.join("workflow", "notebooks", "sample_properties.Rmd")
    output: 
        multiext(notebooks["sample_properties"], ".nb.html", ".pdf", ".md"), 
        samp_list = filtered_sample_list, 
        samp_metadata = filtered_sample_metadata, 
        sites = unfiltered_sites
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """
# Filter samples based on various factors investigated in the previous 
# step
malariagen_filtered_by_samp = os.path.join(
    malariagen_prep_dir, 
    "filtered_by_samp.vcf.gz")
rule filter_samps:
    input:
        vcf = malariagen_filtered_by_snpqc, 
        samp_list = filtered_sample_list
    output: malariagen_filtered_by_samp
    shell: "bcftools view -S {input.samp_list} -o {output} {input.vcf}"

# Get haplotype sequences by applying variants to reference genome
ref_w_variants = "{samp}_ref_w_variants.fa"
# Apply variants to unfiltered reference genome
rule apply_variants:
    input:
        malariagen_filtered_by_samp + ".csi", 
        vcf = malariagen_filtered_by_samp, 
        ref = ref_genome
    output: temp(ref_w_variants)
    shell:
        # Note that this method of obtaining the haplotype sequence 
        # ignores the read depth of each allele, which could be a 
        # limitation. However, the MalariaGEN authors have used a 
        # diploid VCF to encode situations where the read depths of the 
        # reference and alternate alleles are somewhat close as 
        # heterozygous calls (i.e., GT is 0/1). These calls are 
        # replaced with the appropriate IUPAC ambiguity code using the 
        # -I flag. Note that in this case this yields the same results 
        # as the -H I argument (I verified this).
        "bcftools consensus -s {wildcards.samp} -I -f {input.ref} {input.vcf} "
        "> {output}"
mg_microhap_fasta_dir = os.path.join(mg_microhap_dir, "fasta")
hapseq = os.path.join(mg_microhap_fasta_dir, "{samp}_hapseq.fa")
# Filter to the loci of interest
rule filter2hapseq:
    input: 
        fasta = ref_w_variants, 
        bed = trg_coords_good_amp
    output: hapseq
    # Index files for the input fasta are automatically generated
    shadow: "full"
    shell:
        "bedtools getfasta -fi {input.fasta} -bed {input.bed} -fo {output}"
# Finally, convert the data to tidy format and join metadata
mg_microhap_tidy = os.path.join(mg_microhap_dir, "mg_microhap.csv")
mg_microhap_aligned = os.path.join(mg_microhap_dir, "mg_microhap_aligned.rds")
rule tidy_microhap:
    input:
        lambda wildcards: expand_w_samp_list(wildcards, hapseq), 
        trg_coords = trg_coords_good_amp, 
        sample_metadata = filtered_sample_metadata, 
        code = os.path.join("workflow", "scripts", "tidy_microhap.R")
    output: 
        # Unfortunately there is no good alternative to specifying the 
        # alignment FASTAs as a directory. The loci list depends on 
        # assess_marker_performance, and so cannot be expanded in 
        # advance, and I'm not aware of a way to tell Snakemake that 
        # one execution of a rule can create multiple wildcard outputs.
        directory(mg_microhap_aligned_fasta_dir), 
        aligned_haps = mg_microhap_aligned, 
        mh_csv = mg_microhap_tidy
    resources:
        time = 10, 
        mem_mb = 4000
    threads: 1
    benchmark: os.path.join("benchmarks", "tidy_microhap.tsv")
    shell: 
        # Snakemake does not create the aligned FASTA directory 
        # automatically. I gather this is by design when you use a 
        # directory as output - it expects the rule itself to create 
        # it.
        """
        mkdir {mg_microhap_aligned_fasta_dir}
        Rscript {input.code} --mh_fasta_dir {mg_microhap_fasta_dir} \
            --trg_coords {input.trg_coords} --sample_metadata \
            {input.sample_metadata} --aligned_haps {output.aligned_haps} \
            --out_csv {output.mh_csv} --alignments_fasta_dir \
            {mg_microhap_aligned_fasta_dir}
        """

# Linkage disequilibrium
# Convert tibble of msa alignments to pegas haplotypes and add 
# haplotype IDs to the main CSV file
mg_microhap_haplo = os.path.join(mg_microhap_dir, "mg_microhap_haplo.rds")
mg_microhap_w_hap_ids = os.path.join(
    mg_microhap_dir, 
    "mg_microhap_w_hap_ids.csv")
rule alignment2hap:
    input:
        mh_aligned = mg_microhap_aligned, 
        mh_csv = mg_microhap_tidy, 
        code = os.path.join("workflow", "scripts", "alignment2hap.R")
    output: 
        mh_haplo = mg_microhap_haplo, 
        mh_csv_w_hap_ids = mg_microhap_w_hap_ids
    shell:
        "Rscript {input.code} --alignments {input.mh_aligned} --mh_csv "
        "{input.mh_csv} --haplotypes {output.mh_haplo} --mh_csv_w_hap_ids "
        "{output.mh_csv_w_hap_ids}"
rule csv2genind:
    input:
        mh_csv = os.path.join(mg_microhap_dir, "{mh_file}.csv"), 
        code = os.path.join("workflow", "scripts", "csv2genind.R")
    output: os.path.join(mg_microhap_dir, "{mh_file}_gd.rds")
    shell: "Rscript {input.code} --mh_csv {input.mh_csv} --mh_gd {output}"
ld_res = os.path.join("results", "ld_res.rds")
seed = 59404051
mg_microhap_gd = os.path.join(mg_microhap_dir, "mg_microhap_w_hap_ids_gd.rds")
rule check_ld:
    input:
        mh_data = mg_microhap_gd, 
        code = os.path.join("workflow", "scripts", "check_ld.R")
    output: ld_res
    resources:
        time = 60, 
        mem_mb = 2000
    threads: 1
    benchmark: os.path.join("benchmarks", "check_ld.tsv")
    shell:
        "Rscript {input.code} --data_gd {input.mh_data} --seed {seed} --out "
        "{output}"
rule assess_ld:
    input:
        ld_res, 
        code = os.path.join("workflow", "notebooks", "ld.Rmd")
    output:
        multiext(notebooks["ld"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

# Selection
# Convert the msa alignment objects to ape DNAbin objects, subset to 
# have one for each MalariaGEN population
mh_popbin = os.path.join(mg_microhap_dir, "mg_microhap_popbin.rds")
rule alignment2popbin:
    input:
        mh_aligned = mg_microhap_aligned, 
        mh_csv = mg_microhap_tidy, 
        code = os.path.join("workflow", "scripts", "alignment2popbin.R")
    output: mh_popbin
    resources:
        time = 5, 
        mem_mb = 1000
    threads: 1
    benchmark: os.path.join("benchmarks", "alignment2popbin.tsv")
    shell:
        "Rscript {input.code} --alignments {input.mh_aligned} --mh_csv "
        "{input.mh_csv} --out {output}"
loci2filter_selec = os.path.join(loci2filter_dir, "selec.csv")
rule assess_selection:
    input:
        mh_popbin, 
        code = os.path.join("workflow", "notebooks", "selection.Rmd")
    output:
        multiext(notebooks["selection"], ".nb.html", ".pdf", ".md"), 
        loci2filter_selec
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

# Filter data for selection
mg_microhap_filtered4popgen = os.path.join(
    mg_microhap_dir, 
    "mg_microhap_filtered4popgen.csv")
rule filter4popgen:
    input:
        mh_in = mg_microhap_w_hap_ids, 
        loci2filter = loci2filter_selec, 
        code = os.path.join("workflow", "scripts", "filter4popgen.R")
    output: mg_microhap_filtered4popgen
    shell:
        "Rscript {input.code} --mh_in {input.mh_in} --loci2filter "
        "{input.loci2filter} --mh_out {output}"

# Locus and sample diversity
mg_microhap_filtered4popgen_gd = os.path.join(
    mg_microhap_dir, 
    "mg_microhap_filtered4popgen_gd.rds")
rule estimate_diversity:
    input:
        mg_microhap_haplo, 
        mg_microhap_filtered4popgen_gd, 
        mg_microhap_tidy, 
        code = os.path.join("workflow", "notebooks", "diversity.Rmd")
    output: 
        multiext(notebooks["diversity"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# IBD-based relatedness
rel_mat = os.path.join("results", "rel_mat.rds")
rule compute_rel:
    input:
        mh_data = mg_microhap_filtered4popgen, 
        code = os.path.join("workflow", "scripts", "compute_rel.R")
    resources: 
        time = 10, 
        mem_mb = 2000
    threads: 1
    benchmark: os.path.join("benchmarks", "compute_rel.tsv")
    output: rel_mat
    shell: 
        "Rscript {input.code} --gen_data {input.mh_data} --seed {seed} "
        "--rel_mat {output}"
mg_pop_key = os.path.join("resources", "mg_pop_key.csv")
rel_dir = os.path.join("results", "relatedness")
sample_rel = os.path.join(rel_dir, "sample_rel.csv")
site_rel = os.path.join(rel_dir, "site_rel.csv")
country_rel_revcombs = os.path.join(rel_dir, "country_rel_revcombs.csv")
site_rel_revcombs = os.path.join(rel_dir, "site_rel_revcombs.csv")
rule analyze_relatedness:
    input:
        rel_mat, 
        mg_microhap_filtered4popgen, 
        mg_pop_key, 
        code = os.path.join("workflow", "notebooks", "relatedness.Rmd")
    output:
        multiext(notebooks["relatedness"], ".nb.html", ".pdf", ".md"), 
        sample_rel, 
        site_rel, 
        country_rel_revcombs, 
        site_rel_revcombs
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

# Make figures
rule plot_panel_target_metrics:
    input:
        window_stats = os.path.join("resources", "filtered_windows_tab.txt"), 
        targets = loci2filter_good_amp, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "panel_target_metrics_plot.R")
    output: 
        multiext(os.path.join(figdir,figs["panel_target_metrics"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --window_stats {input.window_stats} --targets "
        "{input.targets} --out_base {params.basename}"
may2022_total_readcount = os.path.join(
    "resources", 
    "may2022_totalreadcount.txt")
rule plot_otreads_by_protocol:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        total_readcounts = may2022_total_readcount, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "otreads_by_protocol_barplots.R")
    output: 
        multiext(os.path.join(figdir,figs["otreads_by_protocol"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --total_read_counts "
        "{input.total_readcounts} --out_base {params.basename}"
rule plot_locus_reads_by_protocol:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        selected_loci = loci2filter_good_amp, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "locus_reads_by_protocol_boxplots.R")
    output: 
        multiext(os.path.join(figdir,figs["locus_reads_by_protocol"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --selected_loci {input.selected_loci} "
        "--out_base {params.basename}"
rule plot_otreads_vs_density:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        total_readcounts = may2022_total_readcount, 
        code = os.path.join(
            "workflow", 
            "scripts", 
            "otreads_vs_density_scatterplots.R")
    output: 
        multiext(os.path.join(figdir,figs["otreads_vs_density"]), 
            ".pdf", 
            ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --total_read_counts "
        "{input.total_readcounts} --out_base {params.basename}"
rule plot_reads_by_locus:
    input:
        readcounts_metadata = may2022_readcounts_metadata, 
        selected_loci = loci2filter_good_amp, 
        code = os.path.join("workflow", "scripts", "reads_by_locus_barplots.R")
    output: 
        multiext(os.path.join(figdir,figs["reads_by_locus"]), ".pdf", ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --readcounts_metadata "
        "{input.readcounts_metadata} --selected_loci {input.selected_loci} "
        "--out_base {params.basename}"
rule plot_mean_het_by_pop:
    input:
        mh_data_gd = mg_microhap_filtered4popgen_gd, 
        code = os.path.join("workflow", "scripts", "mean_het_boxplots.R")
    output: 
        multiext(os.path.join(figdir,figs["mean_het_by_pop"]), ".pdf", ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --mh_data_gd {input.mh_data_gd} --out_base "
        "{params.basename}"
rule plot_sample_rel:
    input:
        rel_csv = sample_rel, 
        code = os.path.join("workflow", "scripts", "rel_hist.R")
    output: multiext(os.path.join(figdir,figs["rel_hist"]), ".pdf", ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --rel_csv {input.rel_csv} --out_base "
        "{params.basename}"
heatmap_scale_dict = {
    figs["rel_heatmaps_country"]: country_rel_revcombs, 
    figs["rel_heatmaps_site"]: site_rel_revcombs}
rule rel_heatmaps:
    input:
        rel = lambda wildcards: heatmap_scale_dict[wildcards.fig], 
        code = os.path.join("workflow", "scripts", "rel_heatmaps.R")
    output: multiext(os.path.join(figdir,"{fig}"), ".pdf", ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4], 
        scale = lambda wildcards: 
            os.path.basename(heatmap_scale_dict[wildcards.fig]).split('_')[0]
    wildcard_constraints:
        fig = figs["rel_heatmaps_country"] + '|' + figs["rel_heatmaps_site"]
    shell: 
        "Rscript {input.code} --rel {input.rel} --scale {params.scale} "
        "--out_base {params.basename}"
rule map_rel_net:
    input:
        rel = site_rel, 
        site_coords = unfiltered_sites, 
        code = os.path.join("workflow", "scripts", "network_map.R")
    output: 
        multiext(os.path.join(figdir,figs["site_rel_net"]), ".pdf", ".png")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: 
        "Rscript {input.code} --rel {input.rel} --site_coords "
        "{input.site_coords} --out_base {params.basename}"

# Make PDF with all figures and tables
rule compile_figs_tables:
    input:
        expand(os.path.join(figdir,"{fig}.pdf"), fig=list(figs.values())), 
        code = os.path.join("workflow", "notebooks", "all_figs_tables.Rmd")
    output: figs_tables_doc
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_dir = "results")'
        """

# Copy relevant outputs to shared Google Drive
cloud_dir = "UNCC_GDrive:Alfred-Liz-PV-microhaplotype-files"
rule push_results_notebooks:
    input: os.path.join(rendered_notebook_dir, "{nb}.nb.html")
    output: touch(nb_flag_template)
    priority: -1
    shell: "rclone copy {input} {cloud_dir}/notebooks --drive-shared-with-me"
rule push_figs:
    input: os.path.join(figdir, "{fig}.pdf")
    output: touch(fig_flag_template)
    priority: -1
    shell: "rclone copy {input} {cloud_dir}/figs --drive-shared-with-me"
rule push_figs_tables_doc:
    input: figs_tables_doc
    output: touch(figs_tables_doc_flag)
    priority: -1
    shell: "rclone copy {input} {cloud_dir} --drive-shared-with-me"

# Delete all outputs in preparation for rerunning pipeline from nothing
rule clean:
    shell: "rm -rf results benchmarks logs .flags"
