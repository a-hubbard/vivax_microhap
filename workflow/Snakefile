import glob
import os

# Identify rules to be executed locally
localrules:
    refmt_primers, 
    dwnld_ref, 
    unzip_ref, 
    index_ref_bwa, 
    primers2fastq, 
    find_sa_coords, 
    map_primers, 
    sam2bam, 
    bam2bed, 
    captured_seq_bounds, 
    extract_captured_seq, 
    investigate_mapping, 
    csv2genind, 
    estimate_diversity, 
    assess_ld, 
    enrichment_consistency_plots, 
    dilution_consistency_plots, 
    push_results_notebooks, 
    push_figs, 
    clean

# Notebooks
rendered_notebook_dir = os.path.join("results", "notebooks")
notebooks = {}
notebook_basenames = [
    "investigate_mapping", 
    "diversity", 
    "ld"]
for nb in notebook_basenames:
    notebooks[nb] = os.path.join(rendered_notebook_dir, nb)
nb_flag_template = os.path.join(".flags", "push_nb_{nb}")
# Figures
# All figures, even supplemental ones, are defined here, in the same 
# fashion, to facilitate easy swapping of inserts between supplemental 
# material and the main document
figdir = os.path.join("results", "figs")
figs = {
    "enrichment_consistency_plots": "fig2"}
fig_flag_template = os.path.join(".flags", "push_{fig}")
rule all:
    input:
        expand("{nb}.{ext}", nb=list(notebooks.values()), ext=["pdf","md"]), 
        expand(nb_flag_template, nb=notebook_basenames), 
        expand(os.path.join(figdir,"{fig}.png"), fig=list(figs.values())), 
        expand(fig_flag_template, fig=list(figs.values()))
# For others attempting to run the pipeline on their own machine
rule replicate:
    input:
        expand(
            "{nb}.{ext}", 
            nb=list(notebooks.values()), 
            ext=["nb.html","pdf","md"]), 
        expand(os.path.join(figdir,"{fig}.{ext}"), fig=figs, ext=["png","pdf"])

# Reformat and filter primers
sd_id_file = os.path.join("resources", "id_file.txt")
primer_info = os.path.join(
    "resources", 
    "Microhaplotype primers_fixedRCpf_2023-03-02.xlsx")
primers_tsv = os.path.join("results", "primers.tsv")
fwd_primers = os.path.join("results", "fwd_primers.fasta")
rev_primers = os.path.join("results", "rev_primers.fasta")
rule refmt_primers:
    input:
        sd_id_file = sd_id_file, 
        primer_info = primer_info, 
        code = os.path.join("workflow", "scripts", "refmt_primers.R")
    output:
        primers_tsv = primers_tsv, 
        fwd_primers = fwd_primers, 
        rev_primers = rev_primers
    shell:
        "Rscript {input.code} --sd_id_file {input.sd_id_file} --primer_info "
        "'{input.primer_info}' --primers_tsv {output.primers_tsv} "
        "--fwd_primers {output.fwd_primers} --rev_primers {output.rev_primers}"

# Get primer locations and target reference sequences
# Download reference genome
ref_genome_zip = os.path.join("results", "GCA_900093555.2.zip")
rule dwnld_ref:
    output: temp(ref_genome_zip)
    shell: 
        "datasets download genome accession GCA_900093555.2 --filename {output}"
ref_genome_dir = os.path.join("results", "ref_genome")
ref_genome = os.path.join(
    ref_genome_dir, 
    "GCA_900093555.2", 
    "GCA_900093555.2_GCA_900093555_genomic.fna")
rule unzip_ref:
    input: ref_genome_zip
    output: 
        ref_genome, 
        os.path.join(ref_genome_dir, "README.md"), 
        os.path.join(ref_genome_dir, "assembly_data_report.jsonl"), 
        os.path.join(ref_genome_dir, "dataset_catalog.json")
    shell:
        """
        unzip {input} -d {ref_genome_dir}
        mv {ref_genome_dir}/ncbi_dataset/data/* {ref_genome_dir}/
        rm -r {ref_genome_dir}/ncbi_dataset
        """
# Necessary for mapping
ref_genome_index = multiext(ref_genome, ".amb", ".ann", ".bwt", ".pac", ".sa")
rule index_ref_bwa:
    input: ref_genome
    output: ref_genome_index
    shell: "bwa index {input}"
# Create FASTQ with primer sequences
prim_map_dir = os.path.join("results", "primer_mapping")
primers_fq = os.path.join(prim_map_dir, "primers.fastq")
rule primers2fastq:
    input: primers_tsv
    output: primers_fq
    run:
        with open(input[0], 'r') as primer_list:
            with open(output[0], 'w') as out_fq:
                # Skip header
                primer_list.readline()
                for line in primer_list:
                    elements = line.rstrip().split()
                    target = elements[0]
                    fwd_primer = elements[1]
                    rev_primer = elements[2]
                    out_fq.write("@" + target + "_forward\n")
                    out_fq.write(fwd_primer + '\n')
                    out_fq.write("+\n")
                    out_fq.write("~" * len(fwd_primer) + '\n')
                    out_fq.write("@" + target + "_reverse\n")
                    out_fq.write(rev_primer + '\n')
                    out_fq.write("+\n")
                    out_fq.write("~" * len(rev_primer) + '\n')
# In the next two steps, aln and samse are used to employ the BWA-
# backtrack algorithm, which is meant for sequences less than 100 bp, 
# and especially less than 70 bp
sa_coords = os.path.join(prim_map_dir, "mapping.sai")
rule find_sa_coords:
    input: 
        ref_genome_index, 
        ref = ref_genome, 
        query = primers_fq
    output: sa_coords
    shell: "bwa aln {input.ref} {input.query} > {output}"
primer_mapping_sam = sa_coords[:-3] + "sam"
rule map_primers:
    input:
        ref_genome_index, 
        ref = ref_genome, 
        query = primers_fq, 
        sa_coords = sa_coords
    output: primer_mapping_sam
    shell: "bwa samse {input.ref} {input.sa_coords} {input.query} > {output}"
rule sam2bam:
    input: "{filename}.sam"
    output: temp("{filename}.bam")
    shell: "samtools view -h -b {input} > {output}"
rule bam2bed:
    input: "{filename}.bam"
    output: "{filename}.bed"
    shell: "bedtools bamtobed -i {input} > {output}"
# Convert primer coordinates to target coordinates, both with and 
# without primers
primer_mapping_bed = primer_mapping_sam[:-3] + "bed"
trg_coords_wprimers = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_wprimers.bed")
trg_coords_noprimers = os.path.join(
    prim_map_dir, 
    "captured_seq_coords_noprimers.bed")
rule captured_seq_bounds:
    input: primer_mapping_bed
    output: 
        coords_wprimers = trg_coords_wprimers, 
        coords_noprimers = trg_coords_noprimers
    run:
        with open(input[0], 'r') as primer_coord_f:
            with open(output.coords_wprimers, 'w') as trg_coord_wprimers_f, \
                    open(output.coords_noprimers, 'w') as trg_coord_noprimers_f:
                # The following logic assumes the bed file is sorted 
                # such that forward primers are before reverse primers 
                # and all hits for a given primer pair are grouped 
                # together. Due to the small size of the bed file in 
                # question, this assumption was only verified by 
                # visual inspection.
                chrom = ""
                target = ""
                for line in primer_coord_f:
                    elements = line.rstrip().split()
                    if elements[3].split('_')[-1] == "forward":
                        if elements[3][:-8] == target:
                            raise ValueError(
                                "Forward primer for " + elements[3][:-8] +\
                                " is a duplicate")
                        chrom = elements[0]
                        target = elements[3][:-8]
                        fwd_start = int(elements[1])
                        fwd_end = int(elements[2])
                    if elements[3].split('_')[-1] == "reverse":
                        if elements[3][:-8] != target:
                            raise ValueError(
                                "Reverse primer for " + elements[3][:-8] +\
                                " does not match previous forward primer, " +\
                                "or is a duplicate")
                        elif elements[0] != chrom:
                            raise ValueError(
                                "Forward and reverse primer hits for " +\
                                elements[3][:-8] +\
                                " are on different chromosomes")
                        rev_start = int(elements[1])
                        rev_end = int(elements[2])
                        if fwd_start < rev_start:
                            start_pos_wprimers = str(fwd_start)
                            end_pos_wprimers = str(rev_end)
                            start_pos_noprimers = str(fwd_end)
                            end_pos_noprimers = str(rev_start)
                        else:
                            start_pos_wprimers = str(rev_start)
                            end_pos_wprimers = str(fwd_end)
                            start_pos_noprimers = str(rev_end)
                            end_pos_noprimers = str(fwd_start)
                        trg_coord_wprimers_f.write(
                            '\t'.join([
                                chrom,
                                start_pos_wprimers,
                                end_pos_wprimers,
                                target+'\n']))
                        trg_coord_noprimers_f.write(
                            '\t'.join([
                                chrom,
                                start_pos_noprimers,
                                end_pos_noprimers,
                                target+'\n']))
                        chrom = ""
                        target = ""

# Extract sequence captured by primers from reference
captured_seq = os.path.join(prim_map_dir, "captured_seq.fasta")
rule extract_captured_seq:
    input: 
        ref_genome_index, 
        ref = ref_genome, 
        bed = trg_coords_noprimers
    output: captured_seq
    shell: 
        "bedtools getfasta -fi {input.ref} -bed {input.bed} -nameOnly > "
        "{output}"
# Check mapping for issues
rule investigate_mapping:
    input:
        primers_tsv, 
        trg_coords_noprimers, 
        code = os.path.join("workflow", "notebooks", "investigate_mapping.Rmd")
    output: 
        multiext(notebooks["investigate_mapping"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Compute measures of genetic diversity
rule csv2genind:
    input:
        mh_csv = "{mh_dataset}.csv", 
        code = os.path.join("workflow", "scripts", "csv2genind.R")
    output: "{mh_dataset}_gd.rds"
    shell: "Rscript {input.code} --mh_csv {input.mh_csv} --mh_gd {output}"
microhap_undiluted_gd = os.path.join(microhap_dir, "microhap_undiluted_gd.rds")
sd_moi = os.path.join("resources", "vera-moi_out.tsv")
rule estimate_diversity:
    input:
        microhap_undiluted, 
        microhap_undiluted_gd, 
        asv_fastas, 
        sd_moi, 
        code = os.path.join("workflow", "notebooks", "diversity.Rmd")
    output: 
        multiext(notebooks["diversity"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
           output_dir = "{rendered_notebook_dir}")'
        """

# Check for linkage disequilibrium
rule assess_ld:
    input:
        ld_res_poppr, 
        microhap_filtered4ld_gd, 
        microhap_filtered4ld, 
        code = os.path.join("workflow", "notebooks", "ld.Rmd")
    output:
        multiext(notebooks["ld"], ".nb.html", ".pdf", ".md")
    shell:
        """
        R -e 'rmarkdown::render("{input.code}", output_format = "all", \
            output_dir = "{rendered_notebook_dir}")'
        """

trgs2filter_dir = os.path.join("results", "trgs2filter")

# Make figures
microhap_combined = os.path.join(
    "resources", 
    "Microhap_PV_combined-data_2023-06-21")
rule enrichment_consistency_plots:
    input:
        microhap_combined, 
        code = os.path.join("workflow", "scripts", "consistency_check.R")
    output: 
        multiext(os.path.join(figdir,figs["enrichment_consistency_plots"]), 
            ".pdf", 
            ".png"), 
        os.path.join("results", "failed_matches.csv"), 
        os.path.join("results", "m_check.csv"), 
        os.path.join("results", "MOI_dfz_check.csv")
    params:
        basename = lambda wildcards, output: output[0][:-4]
    shell: "Rscript {input.code} --out_base {params.basename}"

# Copy relevant outputs to shared Google Drive
cloud_dir = "UNCC_GDrive:Alfred-Liz-PV-microhaplotype-files"
rule push_results_notebooks:
    input: os.path.join(rendered_notebook_dir, "{nb}.nb.html")
    output: touch(nb_flag_template)
    priority: -1
    shell: "rclone copy {input} {cloud_dir}/notebooks --drive-shared-with-me"
rule push_figs:
    input: os.path.join(figdir, "{fig}.pdf")
    output: touch(fig_flag_template)
    priority: -1
    shell: "rclone copy {input} {cloud_dir}/figs --drive-shared-with-me"

# Delete all outputs in preparation for rerunning pipeline from nothing
rule clean:
    shell: "rm -rf results benchmarks logs .flags"
